# -*- coding: utf-8 -*-
"""
Created on May Thu 4 22:19:26 2023

- 5è½¦é“åœºæ™¯ï¼Œå«æœ‰curriculum learning
- æ— rule-based guidanceï¼Œæ— LSTMï¼Œæ— æ’è½¦æ’å¢™çš„è§„åˆ™é™åˆ¶

stageè®¾è®¡ï¼š
rewardä¸€æ ·ï¼Œæ¯ä¸ªstageéƒ½æœ‰r_tlï¼Œåˆ‡æ¢æ—¶stageåŒºåˆ«è·¨åº¦è¾ƒå°
    - 3æ¡è½¦é“æ˜¯target lanesï¼Œè½¦è¾†å¯†åº¦ä¸ºä½
    - 2æ¡è½¦é“æ˜¯target lanesï¼Œè½¦è¾†å¯†åº¦ä¸ºä½
    - 1æ¡è½¦é“æ˜¯target lanesï¼Œè½¦è¾†å¯†åº¦ä¸ºä½
    - éšæœºtarget dirï¼Œè½¦è¾†å¯†åº¦ä¸ºä½
    - éšæœºtarget dirï¼Œè½¦è¾†å¯†åº¦ä¸ºä¸­
    - éšæœºtarget dirï¼Œè½¦è¾†å¯†åº¦ä¸ºé«˜

rewardæƒé‡ï¼š
    - efficiency [0, 1]
    - safe [-2, 0]
    - comfort [-1, 0]
    - tl [-2, 0]
ä½¿ç”¨rainbow_linearæ¨¡å‹ï¼Œä½¿ç”¨ rule-based guidance

ï¼ˆ1ï¼‰æ¯ä¸ª timestep éƒ½æœ‰ä¸€ä¸ª ToTL in {llc,rlc, lk} ï¼Œè¡¨ç¤ºå¾€ TL çš„å˜é“æ–¹å‘;
    æ¯ä¸ª timestep éƒ½æœ‰ä¸€ä¸ª LCblockï¼ŒæŒ‡åŸå§‹å˜é“åŠ¨ä½œæ–¹å‘(left or right)å‰å10mæ˜¯å¦æœ‰è½¦
ï¼ˆ2ï¼‰rule-based guidance ä½¿ç”¨åœºæ™¯ï¼š
    - è·ç¦» intersection 2a-a çš„è·ç¦»æ—¶ï¼Œnot suitable to leave a target lane
    - è·ç¦» intersection a-0 çš„è·ç¦»æ—¶ï¼Œurgent need to act as ğ‘‡ğ‘œğ‘‡ ğ¿

ï¼ˆ3ï¼‰RG ä¸º True æ—¶ï¼Œå¹¶ä¸”å½“ToTL æ˜¯ llc æˆ–è€… rlc æ—¶ï¼Œ
    - åˆ¤æ–­ ToTLcleanï¼Œå³å˜é“æ–¹å‘æ˜¯å¦clean
    - ToTLä¾§æ–¹æœ‰è½¦æ—¶ï¼Œä¸èƒ½å˜é“
    - å½“ RG == True and ToTLclean == True ä¿®æ”¹å˜é“åŠ¨ä½œä¸º ToTLï¼Œä¿®æ”¹å¯¹åº”åŠ é€Ÿåº¦
    - å½“ RG == False and LCblock == True ä¿®æ”¹å˜é“åŠ¨ä½œä¸º keepï¼Œä¿®æ”¹å¯¹åº”åŠ é€Ÿåº¦

ä½¿ç”¨é«˜å¯†åº¦
cfg_CL2_high.sumocfg

@author: Simone
"""


import numpy as np
import argparse
from copy import deepcopy
import matplotlib.pyplot as plt
import torch
import random
import os, sys, shutil
import pandas as pd
import math
import pprint as pp
import multiprocessing as mp
from multiprocessing import Process, Queue, Pipe, connection, Lock
curPath=os.path.abspath(os.path.dirname(__file__))
rootPath=os.path.split(os.path.split(curPath)[0])[0]
sys.path.append(rootPath+'/sumo_test01')

#from model.pdqn_model_5tl_lstm import PDQNAgent
from model.pdqn_model_5tl_rainbow_linear import PDQNAgent


# å¼•å…¥åœ°å€ 
sumo_path = os.environ['SUMO_HOME'] # "D:\\sumo\\sumo1.13.0"
# sumo_dir = "C:\--codeplace--\sumo_inter\sumo_test01\sumo\\" # 1.åœ¨æœ¬åœ°ç”¨è¿™ä¸ªcfg_path
#sumo_dir = "D:\Git\MAY1\sumo\\" # 1.åœ¨æœ¬åœ°ç”¨è¿™ä¸ªcfg_path
sumo_dir = "/data1/zengximu/sumo_test01/sumo/" # 2. åœ¨æœåŠ¡å™¨ä¸Šç”¨è¿™ä¸ªcfg_path
OUT_DIR="result_pdqn_5l_cl2_rg2_rainbow_linear_mp"
sys.path.append(sumo_path)
sys.path.append(sumo_path + "/tools")
sys.path.append(sumo_path + "/tools/xml")
import traci # åœ¨ubuntuä¸­ï¼Œtraciå’Œsumolibéœ€è¦åœ¨toolsåœ°å€å¼•å…¥ä¹‹åimport
from sumolib import checkBinary

TRAIN = True # False True
gui = False # False True # æ˜¯å¦æ‰“å¼€gui
if gui == 1:
    sumoBinary = checkBinary('sumo-gui')
else:
    sumoBinary = checkBinary('sumo')

cols = ['stage','epo', 'train_step', 'position_y', 'target_direc', 'lane', 'speed', 
         'lc_int', 'fact_acc', 'acc', 'change_lane', 'r','r_safe', 'r_eff',
         'r_com', 'r_tl', 'r_fluc','other_record', 'done', 's', 's_']
df_record = pd.DataFrame(columns = cols) # å­˜å‚¨transitionç­‰ä¿¡æ¯çš„dataframeï¼Œæ¯ä¸ªepoå»ºç«‹ä¸€ä¸ªdataframe

np.random.seed(0)
random.seed(0)
torch.manual_seed(5)

# PRE_LANE = None
RL_CONTROL = 1100 # Rl agent take control after 1100 meters
UPDATE_FREQ = 100 # model update frequency for multiprocess
DEVICE = torch.device("cuda:3")
# DEVICE = torch.device("cpu")

def get_all(control_vehicle, select_dis):
    """
    è¯¥å‡½æ•°éƒ¨åˆ†ä»£ç ç¨å¾®æ”¹äº†ä¸€ä¸‹é€»è¾‘
    :param control_vehicle: è‡ªåŠ¨é©¾é©¶è½¦çš„ID
    :param select_dis: æ¢æµ‹è½¦è¾†çš„èŒƒå›´ï¼Œ[-select_dis, select_dis]
    :return: Id_list, rel_up, Id_dict å‘¨å›´è½¦ä¿¡æ¯ï¼Œç›¸å¯¹è·ç¦»ï¼Œå‘¨å›´è½¦çš„id
    """
    # è·å–è‡ªåŠ¨é©¾é©¶è½¦çš„Yåæ ‡å’ŒXåæ ‡å’Œçºµå‘é€Ÿåº¦å’Œè½¦é“
    y_pos = traci.vehicle.getPosition(control_vehicle)[0]
    x_pos = traci.vehicle.getPosition(control_vehicle)[1]
    y_speed = traci.vehicle.getSpeed(control_vehicle)
    ego_lane = traci.vehicle.getLaneID(control_vehicle)
    
    # å°†è‡ªåŠ¨é©¾é©¶è½¦è¦ç”¨åˆ°çš„å‘¨å›´è½¦çš„ä¿¡æ¯ä¿å­˜åœ¨map_veå­—å…¸é‡Œï¼Œkeyæ˜¯å‘¨å›´è½¦è¾†çš„IDï¼Œæ¯ä¸ªå‘¨å›´è½¦è¾†çš„æ•°æ®ä¸º
    # [è¯¥è½¦ç›¸å¯¹äºè‡ªåŠ¨é©¾é©¶è½¦çš„ç›¸å¯¹çºµå‘è·ç¦»ï¼Œç›¸å¯¹æ¨ªå‘è·ç¦»ï¼Œç›¸å¯¹é€Ÿåº¦]
    map_ve = {} # è®°å½•è½¦è¾†ä¿¡æ¯
    vehicle_list = traci.vehicle.getIDList()
    for v in vehicle_list:
        if v not in map_ve.keys():
            map_ve[v]=[traci.vehicle.getPosition(v)[0] - y_pos,
                             traci.vehicle.getPosition(v)[1] - x_pos,
                             traci.vehicle.getSpeed(v) - y_speed]
        else:
            map_ve[v]=[traci.vehicle.getPosition(v)[0] - y_pos,
                             traci.vehicle.getPosition(v)[1] - x_pos,
                             traci.vehicle.getSpeed(v) - y_speed]

    # æ€»çš„éœ€è¦æ”¶é›†çš„è½¦è¾†ä¿¡æ¯çš„æ•°ç»„
    Id_list = []
    Id_dict = {}
    # å…­ä¸ªæ–¹å‘çš„è½¦è¾†ä¿¡æ¯ï¼Œè¿™é‡Œæ”¶é›†çš„ä¿¡æ¯ä¸»è¦æ˜¯ä¸ºäº†æ ¹æ®è·ç¦»æ‰¾åˆ°æœ€è¿‘çš„è½¦çš„ID
    up = []
    upright = []
    upleft = []
    down = []
    downright = []
    downleft = []

    # å°†å…­ä¸ªæ–¹å‘çš„è½¦æ”¶é›†åˆ°åˆ†åˆ«çš„æ•°ç»„é‡Œé¢å»
    for v in vehicle_list:
        if v == control_vehicle:
            continue
        # æ­£å‰æ–¹
        if y_pos + select_dis > traci.vehicle.getPosition(v)[0] > y_pos and np.abs(traci.vehicle.getPosition(v)[1]-x_pos) < 1:
            up.append([traci.vehicle.getPosition(v)[0] - y_pos, v])
        # å³å‰æ–¹
        if y_pos + select_dis > traci.vehicle.getPosition(v)[0] > y_pos and 2 < x_pos - traci.vehicle.getPosition(v)[1] < 4:
            upright.append([traci.vehicle.getPosition(v)[0] - y_pos, v])
        # å·¦å‰æ–¹
        if y_pos + select_dis > traci.vehicle.getPosition(v)[0] > y_pos and 2 < traci.vehicle.getPosition(v)[1]-x_pos < 4:
            upleft.append([traci.vehicle.getPosition(v)[0] - y_pos, v])
        # æ­£åæ–¹ï¼Œé˜²æ­¢select_disèŒƒå›´å†…æ²¡æœ‰åè½¦ï¼Œæ‰€æœ‰ä¸è®¾èŒƒå›´
        if traci.vehicle.getPosition(v)[0] < y_pos and np.abs(traci.vehicle.getPosition(v)[1]-x_pos) < 1:
            down.append([traci.vehicle.getPosition(v)[0] - y_pos, v])
        # å³åæ–¹
        if y_pos - select_dis < traci.vehicle.getPosition(v)[0] < y_pos and 2 < x_pos - traci.vehicle.getPosition(v)[1] < 4:
            downright.append([traci.vehicle.getPosition(v)[0] - y_pos, v])
        # å·¦åæ–¹
        if y_pos - select_dis < traci.vehicle.getPosition(v)[0] < y_pos and 2 < traci.vehicle.getPosition(v)[1]-x_pos < 4:
            downleft.append([traci.vehicle.getPosition(v)[0] - y_pos, v])

    # æ’åºæ“ä½œæ˜¯ä¸ºäº†æ‰¾åˆ°é‚£ä¸ªæ–¹å‘ç¦»è‡ªåŠ¨é©¾é©¶è½¦æœ€è¿‘çš„é‚£è¾†è½¦ï¼Œï¼ˆæ¯ä¸ªæ–¹å‘åªé€‰äº†æœ€è¿‘çš„é‚£è¾†ï¼‰
    up = sorted(up, key=lambda x: x[0])
    # å¦‚æœè¯¥æ–¹å‘æœ‰è½¦ åˆ™å°†æœ€è¿‘çš„è½¦çš„ä¿¡æ¯åŠ åˆ°ID_listé‡Œé¢é‡Œé¢ã€‚
    # å¦‚æœè¯¥æ–¹å‘æ²¡è½¦ï¼Œå¯¹äºè‡ªå·±è½¦é“ä¸Šçš„åˆ™ç”¨è¿œçš„maskè½¦è¾†ä»£æ›¿ï¼Œå¦‚æœæ˜¯æ—è¾¹è½¦é“çš„å¾—åˆ¤æ–­åˆ°åº•æœ‰æ²¡æœ‰æ—è¾¹è½¦é“ï¼Œ
    # æœ‰çš„è¯ä¹Ÿæ˜¯ç”¨è¿œçš„maskæ¥ä»£æ›¿ï¼Œå¦åˆ™ç”¨çºµå‘è·ç¦»ä¸º0çš„è½¦è¾†ä»£æ›¿ï¼Œè¡¨ç¤ºè¿™ä¸ªæ–¹å‘æ²¡è½¦é“ï¼Œä¸èƒ½å»
    # up[0][-1]æŒ‡è½¦è¾†ID, map_ve[up[0][-1]]æ˜¯è¦æ”¶é›†çš„è½¦è¾†ä¿¡æ¯
    if len(up) >= 1:
        Id_list.append(map_ve[up[0][-1]])
        Id_dict['up'] = up[0][-1]
    else:
        Id_list.append([200, 0, 0])
        Id_dict['up'] = ''

    upright = sorted(upright, key=lambda x: x[0])
    if len(upright) >= 1:
        Id_list.append(map_ve[upright[0][-1]])
        Id_dict['upright'] = upright[0][-1]
    else:
        if '0' in ego_lane:
            Id_list.append([0, -3.2, 0])
            Id_dict['upright'] = ''
        else:
            Id_list.append([200, -3.2, 0])
            Id_dict['upright'] = ''

    upleft = sorted(upleft, key=lambda x: x[0])
    if len(upleft) >= 1:
        Id_list.append(map_ve[upleft[0][-1]])
        Id_dict['upleft'] = upleft[0][-1]
    else:
        if '4' in ego_lane:
            Id_list.append([0, 3.2, 0])
            Id_dict['upleft'] = ''
        else:
            Id_list.append([200, 3.2, 0])
            Id_dict['upleft'] = ''

    down = sorted(down, key=lambda x: x[0])
    if len(down) >= 1:
        Id_list.append(map_ve[down[-1][-1]])
        Id_dict['down'] = down[-1][-1]
    else:
        Id_list.append([-200, 0, 0])
        Id_dict['down'] = ''

    downright = sorted(downright, key=lambda x: x[0])
    if len(downright) >= 1:
        Id_list.append(map_ve[downright[-1][-1]])
        Id_dict['downright'] = downright[-1][-1]
    else:
        if '0' in ego_lane:
            Id_list.append([0, -3.2, 0])
            Id_dict['downright'] = ''
        else:
            Id_list.append([-200, -3.2, 0])
            Id_dict['downright'] = ''

    downleft = sorted(downleft, key=lambda x: x[0])
    if len(downleft) >= 1:
        Id_list.append(map_ve[downleft[-1][-1]])
        Id_dict['downleft'] = downleft[-1][-1]
    else:
        if '4' in ego_lane:
            Id_list.append([0, 3.2, 0])
            Id_dict['downleft'] = ''
        else:
            Id_list.append([-200, 3.2, 0])
            Id_dict['downleft'] = ''

    # # å¾—åˆ°è‡ªåŠ¨é©¾é©¶è½¦è‡ªå·±æ‰€åœ¨çš„è½¦é“
    if '0' in ego_lane:
        ego_l=-1.0
    elif '1' in ego_lane:
        ego_l=-0.5
    elif '2' in ego_lane:
        ego_l=0
    elif '3' in ego_lane:
        ego_l=0.5
    elif '4' in ego_lane:
        ego_l=1
    else:
        ego_l=5
        print("CODE LOGIC ERROR!")

    # Id_list.append([y_speed/25, cal_a/3, ego_l]) # å…¶ä»–è®ºæ–‡é‡Œä¹Ÿæ˜¯è¿™æ ·åŠ egoè½¦è¾†æ•°æ®
    Id_list.append([y_pos/3100, ego_l, y_speed/25])

    # å½’ä¸€åŒ–ï¼Œéœ€è¦ï¼Œè¦ä¸ç„¶å®¹æ˜“è¾¹ç•Œå€¼
    for i in range(6):
        Id_list[i][0] = Id_list[i][0]/select_dis
        Id_list[i][1] = Id_list[i][1]/3.2
        Id_list[i][2] = Id_list[i][2]/25

    # ä¸ºäº†å¾—åˆ°ä¸å‰è½¦çš„ç›¸å¯¹çºµå‘è·ç¦»å’Œç›¸å¯¹çºµå‘é€Ÿåº¦ï¼Œå‰æ–¹è½¦è¾†å¯èƒ½ä¸åœ¨200må†…ï¼Œä¸èƒ½ç›´æ¥ç”¨upä¸­çš„æ•°æ®
    relspace = 100000
    relspeed = 0
    for v in vehicle_list:
        if v == control_vehicle:
            continue
        if traci.vehicle.getPosition(v)[0] - y_pos > 0 and traci.vehicle.getLaneID(v) == ego_lane:
            if traci.vehicle.getPosition(v)[0] - y_pos < relspace:
                relspace = traci.vehicle.getPosition(v)[0] - y_pos - 5 #vehicle-length: 5
                relspeed = traci.vehicle.getSpeed(v) - y_speed
    
    rel_up = {'relspace': relspace, 'relspeed':relspeed}
    
    # å‘¨å›´è½¦çš„3ä¸ªç›¸å¯¹ä¿¡æ¯ï¼Œç›¸å¯¹è·ç¦»ï¼Œå‘¨å›´è½¦åˆ†æ–¹å‘è®°å½•id
    return Id_list, rel_up, Id_dict


def train(worker, lock, traj_q, agent_q, control_vehicle, episode, target_dir, CL_Stage):
    '''
    - get_allè·å¾—å‘¨å›´ä¿¡æ¯å’ŒåŠ¨ä½œä¿¡æ¯
    - choose_action å¾—åˆ°è¿”å›åŠ¨ä½œ ret_action_lc_int, ret_action_acc
    - è®°å½•preæ•°æ®
    - rule-based guidance è·å¾—ToTLï¼ŒRGå’ŒToTLcleanä¿®æ”¹change_lane, action_acc
    - æ ¹æ® change_laneåˆ¤æ–­æ˜¯å¦æ’å¢™ï¼Œè‹¥æ’å¢™ï¼Œç»“æŸå›åˆ
    - æ ¹æ®change_lane, action_accå˜é“å˜é€Ÿ
    - æ‰§è¡Œï¼ŒsimulateionStep
    - è®°å½•curæ•°æ®
    - è®¡ç®—reward
    - æŸ¥è¯¢æ˜¯å¦å‘ç”Ÿç¢°æ’

    :return: collision 
    '''
    print()
    print(f"+++++++++++++++ epo: {episode}  CL_Stage: {CL_Stage} ++++++++++++++++")
    print(f"++++++++++++++ {OUT_DIR} ++++++++++++++++")
    global TRAIN
    global df_record
    stage = CL_Stage # å½“å‰trainæ˜¯åœ¨å“ªä¸ªCLçš„stage
    
    # 1. get_allè·å¾—å‘¨å›´ä¿¡æ¯å’ŒåŠ¨ä½œä¿¡æ¯
    detect_dis = 200 # egoè½¦è¾†çš„æ¢æµ‹è·ç¦»
    all_vehicle, rel_up, v_dict = get_all(control_vehicle, detect_dis)
    print("$ v_dict", v_dict)
    
    # target_dir_inits ç¼–ç 
    tl_list = [[0,1,0,0,0,0,1], [1,1,0,1,1,1,0], [1,0,1,1,0,0,0]] # 0 æ˜¯å‰æ–¹å³è½¬ï¼Œ1æ˜¯ç›´è¡Œï¼Œ2æ˜¯å‰æ–¹å·¦è½¬
    tl_code = tl_list[target_dir] # æ ¹æ®target_dirè·å¾—å¯¹åº”directionçš„tl_code
    
    # 2. choose_action å¾—åˆ°è¿”å›åŠ¨ä½œ ret_action_lc_int, ret_action_acc
    if TRAIN:
        ret_action_lc_int, ret_action_acc, all_action_parameters = worker.choose_action(np.array(all_vehicle), tl_code) # è¿”å›ç¦»æ•£lane change ï¼Œè¿ç»­accï¼Œå‚æ•°
    else:
        ret_action_lc_int, ret_action_acc, all_action_parameters = worker.choose_action(np.array(all_vehicle), tl_code, train = False)
    
    inf = -10 # æ’å¢™æƒ©ç½š
    inf_car = -10 # æ’è½¦æƒ©ç½š
    done = 0 # å›åˆç»“æŸæ ‡å¿—
    
    action_change_dict = {0: 'left', 1: 'keep', 2:'right'}
    ret_change_lane = action_change_dict[ret_action_lc_int] # 0è½¦é“å³è½¦é“åœ¨-8.0ï¼›1è½¦é“åœ¨-4.8ï¼›2è½¦é“å·¦è½¦é“åœ¨-1.6
        
    # 3. è®°å½•preæ•°æ®
    pre_ego_info_dict = {"speed": traci.vehicle.getSpeed(control_vehicle), 
                         "acc":traci.vehicle.getAcceleration(control_vehicle), 
                         "LaneID": traci.vehicle.getLaneID(control_vehicle),
                         # "LaneIndex": traci.vehicle.getLaneIndex(control_vehicle), 
                         "position": traci.vehicle.getPosition(control_vehicle)}
    print("$ pre_ego_info_dict")
    pp.pprint(pre_ego_info_dict, indent = 5)
    
    get_all_info = [] # è®°å½•ä¸å‰åè½¦çš„è·ç¦»
    get_all_info.append('v_dict')
    
    # egoè·ç¦»6ä¸ªæ–¹å‘è½¦çš„çºµå‘è·ç¦»ï¼Œåˆå§‹åŒ–ä¸ºæœ€è¿œçš„detect_dis
    dis_to_up = detect_dis
    dis_to_upright = detect_dis
    dis_to_upleft = detect_dis
    dis_to_down = detect_dis
    dis_to_downright = detect_dis
    dis_to_downleft = detect_dis

    if v_dict['up'] != '':
        dis_to_up = traci.vehicle.getPosition(v_dict['up'])[0] - pre_ego_info_dict['position'][0]
        get_all_info.append(("up", v_dict['up'], dis_to_up))
    if v_dict['upright'] != '':
        dis_to_upright = traci.vehicle.getPosition(v_dict['upright'])[0] - pre_ego_info_dict['position'][0]
        get_all_info.append(("upright", v_dict['upright'], dis_to_upright))
    if v_dict['upleft'] != '':
        dis_to_upleft = traci.vehicle.getPosition(v_dict['upleft'])[0] - pre_ego_info_dict['position'][0]
        get_all_info.append(("upleft", v_dict['upleft'], dis_to_upleft))
    if v_dict['down'] != '':
        dis_to_down = pre_ego_info_dict['position'][0] - traci.vehicle.getPosition(v_dict['down'])[0]
        get_all_info.append(("down", v_dict['down'], dis_to_down))
    if v_dict['downright'] != '':
        dis_to_downright = pre_ego_info_dict['position'][0] - traci.vehicle.getPosition(v_dict['downright'])[0]
        get_all_info.append(("downright", v_dict['downright'], dis_to_downright))
    if v_dict['downleft'] != '':
        dis_to_downleft = pre_ego_info_dict['position'][0] - traci.vehicle.getPosition(v_dict['downleft'])[0]
        get_all_info.append(("downleft", v_dict['downleft'], dis_to_downleft))

    # 4. rule-based guidance
    ego_laneInt = int(pre_ego_info_dict["LaneID"][-1]) # ä»å·¦åˆ°å³æ˜¯ 4 3 2 1 0
    ToTL = None
    if target_dir == 0: # å‰æ–¹éœ€è¦å³è½¬
        if ego_laneInt != 0:
            ToTL = 2 # right lc
        else:
            ToTL = 1 # lane keeping
    elif target_dir == 1: # å‰æ–¹éœ€è¦ç›´è¡Œ
        if ego_laneInt == 0:
            ToTL = 0 # left lc
        elif ego_laneInt == 4:
            ToTL = 2
        else:
            ToTL = 1
    elif target_dir == 2: # å‰æ–¹éœ€è¦å·¦è½¬
        if ego_laneInt in [0, 1, 2]:
            ToTL = 0
        else:
            ToTL = 1
    
    RG = False
    dis_a = 200 # è·ç¦»intersectionåˆ†è·ç¦»çš„rule
    v_length = 5 # è½¦èº«é•¿åº¦
    # åœ¨ ret_action_lc_int ä¸ ToTL ä¸ä¸€è‡´çš„æƒ…å†µä¸‹ï¼Œæ‰§è¡Œåˆ†è·ç¦»çš„RG
    if ret_action_lc_int != ToTL:
        # è·ç¦» intersection 2a ~ a
        if pre_ego_info_dict['position'][0] >= 3100 - 2 * dis_a and pre_ego_info_dict['position'][0] < 3100 - dis_a:
            RG = True
        # è·ç¦» intersection a ~ 0
        elif pre_ego_info_dict['position'][0] >= 3100 - dis_a:
            RG = True
    
    # åœ¨RGä¸ºTrue çš„æƒ…å†µä¸‹ï¼Œåˆ¤æ–­æ˜¯å¦ ToTLclean
    ToTLclean = False
    if RG == True:
        # left lc
        if ToTL == 0 and dis_to_upleft >= 2 * v_length and dis_to_downleft >= 2 * v_length:
            ToTLclean = True
        # right lc
        if ToTL == 2 and dis_to_upright >= 2 * v_length and dis_to_downright >= 2 * v_length:
            ToTLclean = True
    
    LCblock = False # keep çš„æƒ…å†µä¸‹ä¸éœ€è¦ä¿®æ”¹
    # left lc
    if ret_action_lc_int == 0:
        if dis_to_upleft <= 2 * v_length or dis_to_downleft <= 2 * v_length:
            LCblock = True
    # right lc
    if ret_action_lc_int == 2:
        if dis_to_upright <= 2 * v_length or dis_to_downright <= 2 * v_length:
            LCblock = True
    
    # åœ¨ RG å’Œ ToTLclean åŒæ—¶æˆç«‹çš„æƒ…å†µä¸‹ï¼Œä¿®æ­£åŠ¨ä½œï¼›å¦åˆ™ï¼Œä½¿ç”¨æ¨¡å‹è¿”å›çš„åŠ¨ä½œ
    if RG == True and ToTLclean == True:
        action_lc_int = ToTL
        action_acc = all_action_parameters[action_lc_int]
        change_lane = action_change_dict[action_lc_int]
    # æ²¡æœ‰ RG ä»‹å…¥ï¼Œä½† LCblock æœ‰é˜»ç¢ï¼Œä¿®æ­£å˜é“åŠ¨ä½œä¸º keepï¼Œå’Œå¯¹åº”çš„ acc
    elif RG == False and LCblock == True:
        action_lc_int = 1 # keep
        action_acc = all_action_parameters[action_lc_int]
        change_lane = action_change_dict[action_lc_int]
    else:
        action_lc_int = ret_action_lc_int
        action_acc = ret_action_acc
        change_lane = ret_change_lane
    
    get_all_info.append((('RG', RG), ('ToTLclean', ToTLclean), ('ToTL', ToTL), ('ret_action_lc_int', ret_action_lc_int),
                        ('ret_action_acc', ret_action_acc), ('ret_change_lane', ret_change_lane)))
    
    # 5. æ ¹æ® change_laneåˆ¤æ–­æ˜¯å¦æ’å¢™ï¼Œè‹¥æ’å¢™ï¼Œç»“æŸå›åˆ
    collision=0
    loss_actor = 0
    Q_loss = 0
    if 'EA_0' == pre_ego_info_dict["LaneID"] and change_lane=='right':
        collision=1
        done = 1
        train_step = worker._step
        print(f"\t ====== worker_step:{worker._step} learner_step:{worker._learn_step} target_dir:{target_dir} ======")

        print('$ transition')
        pp.pprint({'obs': all_vehicle, 'act_lc': action_lc_int, 'act_param': all_action_parameters, 
                  'rew': inf, 'next_obs': np.zeros((7,3)), 'done': done}, indent = 5)
        traj_q.put((deepcopy(all_vehicle), deepcopy(tl_code), deepcopy(action_lc_int), deepcopy(all_action_parameters),
                inf, deepcopy(np.zeros((7,3))), deepcopy(tl_code), done), block=True, timeout=None)
        df_record = df_record.append(pd.DataFrame([[stage,episode, train_step, pre_ego_info_dict['position'][0], 
                                                    target_dir, pre_ego_info_dict['LaneID'], 
                                                    pre_ego_info_dict['speed'], action_lc_int, pre_ego_info_dict['acc'], action_acc, change_lane, 
                                                    inf, 0, 0, 0, 0, 0, 0, done, all_vehicle, np.zeros((7,3))]], columns = cols))
        print("====================å³å³å³å³è½¦é“æ’å¢™å¢™å¢™å¢™===================")
        return collision, loss_actor, Q_loss

    if 'EA_4' == pre_ego_info_dict["LaneID"] and change_lane=='left':
        collision=1
        done = 1
        train_step = worker._step
        print(f"\t ====== worker_step:{worker._step} learner_step:{worker._learn_step} target_dir:{target_dir} ======")
        print('$ transition')
        pp.pprint({'obs': all_vehicle, 'act_lc': action_lc_int, 'act_param': all_action_parameters, 
                  'rew': inf, 'next_obs': np.zeros((7,3)), 'done': done}, indent = 5)
        traj_q.put((deepcopy(all_vehicle), deepcopy(tl_code), deepcopy(action_lc_int), deepcopy(all_action_parameters),
                inf, deepcopy(np.zeros((7,3))), deepcopy(tl_code), done), block=True, timeout=None)
        df_record = df_record.append(pd.DataFrame([[stage,episode, train_step, pre_ego_info_dict['position'][0], 
                                                    target_dir, pre_ego_info_dict['LaneID'], 
                                                    pre_ego_info_dict['speed'], action_lc_int, pre_ego_info_dict['acc'], action_acc, change_lane, 
                                                    inf, 0, 0, 0, 0, 0, 0, done, all_vehicle, np.zeros((7,3))]], columns = cols))
        print("====================å·¦å·¦å·¦å·¦è½¦é“æ’å¢™å¢™å¢™å¢™===================")
        return collision, loss_actor, Q_loss
    
    # 6. æ ¹æ®change_lane, action_accå˜é“å˜é€Ÿ
    # è®¡ç®—é€Ÿåº¦ï¼Œæ²¡æœ‰é™é€Ÿæ§åˆ¶
    sp = traci.vehicle.getSpeed(control_vehicle) + action_acc*0.5 # 0.5s simulateä¸€æ¬¡
    traci.vehicle.setSpeed(control_vehicle, sp) # å°†é€Ÿåº¦è®¾ç½®å¥½
    print('$ speed ', sp)
    
    # å˜é“å¤„ç†
    if change_lane=='left':
        if '0' in pre_ego_info_dict["LaneID"]:
            traci.vehicle.moveTo(control_vehicle, 'EA_1', traci.vehicle.getLanePosition(control_vehicle))
        elif '1' in pre_ego_info_dict["LaneID"]:
            traci.vehicle.moveTo(control_vehicle, 'EA_2', traci.vehicle.getLanePosition(control_vehicle))
        elif '2' in pre_ego_info_dict["LaneID"]:
            traci.vehicle.moveTo(control_vehicle, 'EA_3', traci.vehicle.getLanePosition(control_vehicle))
        elif '3' in pre_ego_info_dict["LaneID"]:
            traci.vehicle.moveTo(control_vehicle, 'EA_4', traci.vehicle.getLanePosition(control_vehicle))
    elif change_lane=='right':
        if '1' in pre_ego_info_dict["LaneID"]:
            traci.vehicle.moveTo(control_vehicle, 'EA_0', traci.vehicle.getLanePosition(control_vehicle))
        elif '2' in pre_ego_info_dict["LaneID"]:
            traci.vehicle.moveTo(control_vehicle, 'EA_1', traci.vehicle.getLanePosition(control_vehicle))
        elif '3' in pre_ego_info_dict["LaneID"]:
            traci.vehicle.moveTo(control_vehicle, 'EA_2', traci.vehicle.getLanePosition(control_vehicle))
        elif '4' in pre_ego_info_dict["LaneID"]:
            traci.vehicle.moveTo(control_vehicle, 'EA_3', traci.vehicle.getLanePosition(control_vehicle))    
    
    # 7. æ‰§è¡Œ
    # ================================æ‰§è¡Œ ==================================
    traci.simulationStep()
    train_step = worker._step
    print("\t ################ æ‰§è¡Œ ###################")
    print(f"\t ====== worker_step:{worker._step} learner_step:{worker._learn_step} target_dir:{target_dir} ======")
    
    # 8. è®°å½•curæ•°æ®
    new_all_vehicle, new_rel_up, new_v_dict = get_all(control_vehicle, 200)
    print("$ new_v_dict", new_v_dict)

    cur_ego_info_dict = {"speed": traci.vehicle.getSpeed(control_vehicle), 
                     "acc":traci.vehicle.getAcceleration(control_vehicle), 
                     "LaneID": traci.vehicle.getLaneID(control_vehicle), 
                     # "LaneIndex": traci.vehicle.getLaneIndex(control_vehicle), 
                     "position": traci.vehicle.getPosition(control_vehicle)}
    
    print("$ cur_ego_info_dict")
    pp.pprint(cur_ego_info_dict, indent = 5)
    
    # 9. è®¡ç®—reward
    e = 0.000001 # é¿å…åˆ†æ¯ä¸º0
    if 0 <= new_rel_up['relspeed'] < e:
        new_rel_up['relspeed'] = e
    if -e < new_rel_up['relspeed'] < 0:
        new_rel_up['relspeed'] = -e
    
    y_ttc=-new_rel_up['relspace']/new_rel_up['relspeed'] # time to collision
    max_speed = 25
    if cur_ego_info_dict['speed'] > max_speed:
        r_efficiency = math.exp(max_speed - cur_ego_info_dict['speed'])
    else:
        r_efficiency = cur_ego_info_dict['speed'] / max_speed
    
    if 0 < y_ttc < 8: 
        r_safe = np.log(y_ttc/8)
    else:
        r_safe = 0
    
    if r_safe < -2: # å¯¹r_safe è¿›è¡Œè£å‰ª
        r_safe = -2
        
    r_comfort = ((np.abs(pre_ego_info_dict['acc']-cur_ego_info_dict['acc'])/0.1) ** 2) / 3600 

    r_tl = 0
    if cur_ego_info_dict['LaneID'] != '':
        if target_dir == 0:
            r_tl = -(0.0005 * (pre_ego_info_dict['position'][0] - RL_CONTROL) ) * abs(int(cur_ego_info_dict['LaneID'][-1]) - 0) *1/4
        elif target_dir == 1:
            if int(cur_ego_info_dict['LaneID'][-1]) == 4 or int(cur_ego_info_dict['LaneID'][-1]) == 0:
                r_tl = -(0.0005 * (pre_ego_info_dict['position'][0] - RL_CONTROL) ) *1/4
            else:
                r_tl = 0
        else:
            if int(cur_ego_info_dict['LaneID'][-1]) == 4 or int(cur_ego_info_dict['LaneID'][-1]) == 3:
                r_tl = 0
            else:
                r_tl = -(0.0005 * (pre_ego_info_dict['position'][0] - RL_CONTROL) ) * abs(int(cur_ego_info_dict['LaneID'][-1]) - 3) *1/4

    # # add penalty to discourage lane_change behavior fluctuation
    # if PRE_LANE == None:
    #     r_fluc = 0
    # else:
    #     r_fluc = -abs(cur_ego_info_dict['LaneIndex'] - PRE_LANE) * (1-abs(r_tl)) * 0.1
    # r_fluc=0
    # globals()['PRE_LANE'] = cur_ego_info_dict['LaneIndex']
    r_fluc = 0 # å–æ¶ˆr_flucçš„ä½œç”¨
    
    get_all_info.append("new_v_dict")
    if new_v_dict['up'] != '':
        get_all_info.append(("up", new_v_dict['up'], traci.vehicle.getPosition(new_v_dict['up'])[0] - cur_ego_info_dict['position'][0]))
    if new_v_dict['upright'] != '':
        get_all_info.append(("upright", new_v_dict['upright'], traci.vehicle.getPosition(new_v_dict['upright'])[0] - cur_ego_info_dict['position'][0]))
    if new_v_dict['upleft'] != '':
        get_all_info.append(("upleft", new_v_dict['upleft'], traci.vehicle.getPosition(new_v_dict['upleft'])[0] - cur_ego_info_dict['position'][0]))
    if new_v_dict['down'] != '':
        get_all_info.append(("down", new_v_dict['down'], cur_ego_info_dict['position'][0] - traci.vehicle.getPosition(new_v_dict['down'])[0]))
    if new_v_dict['downright'] != '':
        get_all_info.append(("downright", new_v_dict['downright'], cur_ego_info_dict['position'][0] - traci.vehicle.getPosition(new_v_dict['downright'])[0]))
    if new_v_dict['downleft'] != '':
        get_all_info.append(("downleft", new_v_dict['downleft'], cur_ego_info_dict['position'][0] - traci.vehicle.getPosition(new_v_dict['downleft'])[0]))
    
    cur_reward = r_safe + r_efficiency - r_comfort + r_fluc + r_tl * 2
    # è®¡ç®— bad action çš„ bad rewardï¼Œå¹¶å­˜å‚¨ transition
    # ä¿®æ­£æƒ…å†µ 1 
    if RG == True and ToTLclean == True:
        bad_reward = cur_reward - 0.5 * abs(action_lc_int - ret_action_lc_int) - abs(action_acc-ret_action_acc)
        done = 1
        traj_q.put((deepcopy(all_vehicle), deepcopy(tl_code), deepcopy(ret_action_lc_int), deepcopy(all_action_parameters),\
            bad_reward, deepcopy(np.zeros((7,3))), deepcopy(tl_code), done), block=True, timeout=None)
        df_record = df_record.append(pd.DataFrame([[stage,episode, train_step, cur_ego_info_dict['position'][0], 
                                                    target_dir, cur_ego_info_dict['LaneID'], 
                                                    cur_ego_info_dict['speed'], action_lc_int, cur_ego_info_dict['acc'], ret_action_lc_int, ret_change_lane,
                                                    bad_reward, 0, 0, 0, 0, 0, get_all_info, done, 
                                                    all_vehicle, np.zeros((7,3))]], columns = cols))
        done = 0 # æ”¹æˆ 0 ç»§ç»­è·‘ï¼Œå¦åˆ™ revised actionä¸­çš„done ä¹Ÿä¼šæ˜¯1
    # ä¿®æ­£æƒ…å†µ 2
    if RG == False and LCblock == True:
        bad_reward = cur_reward - 0.5 * abs(action_lc_int - ret_action_lc_int) - abs(action_acc-ret_action_acc)
        done = 1
        traj_q.put((deepcopy(all_vehicle), deepcopy(tl_code), deepcopy(ret_action_lc_int), deepcopy(all_action_parameters),\
            bad_reward, deepcopy(np.zeros((7,3))), deepcopy(tl_code), done), block=True, timeout=None)
        df_record = df_record.append(pd.DataFrame([[stage,episode, train_step, cur_ego_info_dict['position'][0], 
                                                    target_dir, cur_ego_info_dict['LaneID'], 
                                                    cur_ego_info_dict['speed'], action_lc_int, cur_ego_info_dict['acc'], ret_action_lc_int, ret_change_lane,
                                                    bad_reward, 0, 0, 0, 0, 0, get_all_info, done, 
                                                    all_vehicle, np.zeros((7,3))]], columns = cols))
        done = 0 # æ”¹æˆ 0 ç»§ç»­è·‘ï¼Œå¦åˆ™ revised actionä¸­çš„done ä¹Ÿä¼šæ˜¯1
    
    # 10. æŸ¥è¯¢è‡ªåŠ¨é©¾é©¶è½¦æ˜¯å¦å‘ç”Ÿç¢°æ’
    collision=0
    loss_actor = 0
    Q_loss = 0
    if control_vehicle in traci.simulation.getCollidingVehiclesIDList():
        print('=====================collision==', traci.simulation.getCollidingVehiclesIDList(), control_vehicle) # ç¬¬ä¸€ä¸ªæ˜¯è‡ªåŠ¨é©¾é©¶è½¦è¾†
        print("==========================å‘ç”Ÿäº†æ’è½¦=========================")
        if control_vehicle == traci.simulation.getCollidingVehiclesIDList()[1]:
            print("ä¸å‰æ–¹è½¦è¾†æ’")
            another_co_id = traci.simulation.getCollidingVehiclesIDList()[0]
        elif control_vehicle == traci.simulation.getCollidingVehiclesIDList()[0]:
            print("ä¸åæ–¹è½¦è¾†æ’")
            another_co_id = traci.simulation.getCollidingVehiclesIDList()[1]
        get_all_info.append(("another_co_id", another_co_id))
        print("$ another_co_id ", another_co_id)
        # print("**** collision ego ****", traci.vehicle.getPosition(control_vehicle)[0], # ego
        #       traci.vehicle.getPosition(control_vehicle)[1], 
        #       "pre_ego", pre_ego_info_dict['position'][0], pre_ego_info_dict['position'][1])
        # print("**** collision another****", traci.vehicle.getPosition(another_co_id)[0], # å¦ä¸€ä¸ªç¢°æ’çš„vehicle
        #       traci.vehicle.getPosition(another_co_id)[1])
        collision = 1
        done = 1
        print('$ transition')
        pp.pprint({'obs': all_vehicle, 'act_lc': action_lc_int, 'act_param': all_action_parameters, 
                  'rew': [inf_car, ('r_safe', r_safe), ('r_efficiency', r_efficiency), ('r_comfort', r_comfort), ('r_tl', r_tl)], 
                  'next_obs': new_all_vehicle, 'done': done}, indent = 5)
        traj_q.put((deepcopy(all_vehicle), deepcopy(tl_code), deepcopy(action_lc_int), deepcopy(all_action_parameters),
                inf_car, deepcopy(new_all_vehicle), deepcopy(tl_code), done), block=True, timeout=None)
        df_record = df_record.append(pd.DataFrame([[stage,episode, train_step, cur_ego_info_dict['position'][0], 
                                            target_dir, cur_ego_info_dict['LaneID'], 
                                            cur_ego_info_dict['speed'], action_lc_int, cur_ego_info_dict['acc'], action_acc, change_lane, 
                                            inf_car, r_safe, r_efficiency, r_comfort, r_tl, r_fluc, get_all_info, done, 
                                            all_vehicle, new_all_vehicle]], columns = cols))
        return collision, loss_actor, Q_loss
    

    print('$ transition')
    pp.pprint({'obs': all_vehicle, 'act_lc': action_lc_int, 'act_param': all_action_parameters, 
              'rew': [cur_reward, ('r_safe', r_safe), ('r_efficiency', r_efficiency), ('r_comfort', r_comfort), ('r_tl', r_tl)], 
              'next_obs': new_all_vehicle, 'done': done}, indent = 5)
    traj_q.put((deepcopy(all_vehicle), deepcopy(tl_code), deepcopy(action_lc_int), deepcopy(all_action_parameters),\
                cur_reward, deepcopy(new_all_vehicle), deepcopy(tl_code), done), block=True, timeout=None)
    df_record = df_record.append(pd.DataFrame([[stage,episode, train_step, cur_ego_info_dict['position'][0], 
                                                target_dir, cur_ego_info_dict['LaneID'], 
                                                cur_ego_info_dict['speed'], action_lc_int, cur_ego_info_dict['acc'], action_acc, change_lane,
                                                cur_reward, r_safe, r_efficiency, r_comfort, r_tl, r_fluc, get_all_info, done, 
                                                all_vehicle, new_all_vehicle]], columns = cols))
    
    if TRAIN and not agent_q.empty():
        lock.acquire()
        model_dict=torch.load(f"./{OUT_DIR}/learner.pth", map_location=DEVICE)
        worker.actor.load_state_dict(model_dict["actor"])
        worker.actor_target.load_state_dict(model_dict["actor_target"])
        worker.param.load_state_dict(model_dict["param"])
        worker.param_target.load_state_dict(model_dict["param_target"])
        _learn_step, loss_actor, Q_loss = agent_q.get()
        lock.release()
        worker._learn_step=_learn_step

        print('$ actorçš„loss ', loss_actor, 'qçš„loss ', Q_loss)
    else:
        loss_actor = Q_loss = None
    
    return collision, loss_actor, Q_loss


def main_train():
    a_dim = 1 # one parameter for a continous action
    s_dim = 3 * 7    # ego vehicle + 6 surrounding vehicle
    agent_param={
        "s_dim": s_dim,
        "a_dim": a_dim,
        "acc3": True,
        "Kaiming_normal": False,
        "memory_size": 40000,
        "minimal_size": 5000,
        "batch_size": 128,
        "n_step": 3,
        "per_flag": True,
        "device": DEVICE
    }

    worker = PDQNAgent(
        state_dim=agent_param["s_dim"],
        action_dim=agent_param["a_dim"],
        acc3=agent_param["acc3"],
        Kaiming_normal=agent_param["Kaiming_normal"],
        memory_size=agent_param["memory_size"],
        minimal_size=agent_param["minimal_size"],
        batch_size=agent_param["batch_size"],
        n_step=agent_param["n_step"],
        per_flag=agent_param["per_flag"],
        device=agent_param["device"])
    process=list()
    traj_q=Queue(maxsize=40000)
    agent_q=Queue(maxsize=1)
    lock=Lock()
    process.append(mp.Process(target=learner_process, args=(lock, traj_q, agent_q, deepcopy(agent_param))))
    [p.start() for p in process]

    losses_actor = [] # ä¸éœ€è¦çœ‹ç¬¬ä¸€ä¸ªmemory å³å‰20000æ­¥
    losses_episode = [] # å­˜ä¸€ä¸ªepisodeçš„lossï¼Œä¸€ä¸ªepisodeç»“æŸåæ¸…é™¤å†…å®¹
    switch_cnt = 0 # æŸä¸ªstageä¸­epoçš„æ•°é‡
    swicth_min = 50 # ä¸€ä¸ªstageä¸­æœ€å°‘è¦è®­ç»ƒswicth_minä¸ªepo
        
    # (1) åŒºåˆ†trainå’Œtestçš„å‚æ•°è®¾ç½®ï¼Œä»¥åŠoutputä½ç½®
    if not TRAIN:
        episode_num = 400 # testçš„episodeä¸Šé™
        CL_Stage = 6 # testéƒ½åœ¨æœ€åä¸€ä¸ªstageè¿›è¡Œ
        worker.load_state_dict(torch.load(f"{OUT_DIR}/net_params.pth", map_location=DEVICE))
        globals()['RL_CONTROL']=1100
        globals()['OUT_DIR']=f"./{OUT_DIR}/test"
    else:
        episode_num = 20000 # trainçš„episodeä¸Šé™
        CL_Stage = 1 # trainä»stage 1 å¼€å§‹
        # CL_Stage = 6
        if os.path.exists(f"./model_params/{OUT_DIR}_net_params.pth"): #load pre-trained model params for further training
            worker.load_state_dict(torch.load(f"./model_params/{OUT_DIR}_net_params.pth", map_location=DEVICE)) 
    
    # åˆ›å»ºoutputæ–‡ä»¶å¤¹
    if not os.path.exists(OUT_DIR):
        os.makedirs(OUT_DIR)
    else:
        shutil.rmtree(OUT_DIR, ignore_errors=True)
        os.makedirs(OUT_DIR)
    
    # (2) åˆ†episodeè¿›è¡Œ train / test
    for epo in range(episode_num): 
        truncated = False # æ’è½¦
        target_dir_init = None # åˆå§‹çš„target_dirï¼Œç›®æ ‡è½¬å‘æ–¹å‘
        
        # (3) æ ¹æ®ä¸åŒçš„CL_Stageå¯åŠ¨å¯¹åº”çš„sumoCmd
        if CL_Stage in [1, 2, 3, 4]:
            cfg_path = f"{sumo_dir}cfg_CL2_low.sumocfg"
        elif CL_Stage == 5:
            cfg_path = f"{sumo_dir}cfg_CL2_mid.sumocfg"
        elif CL_Stage == 6:
            cfg_path = f"{sumo_dir}cfg_CL2_high.sumocfg"
        sumoCmd = [sumoBinary, "-c", cfg_path, "--log", f"{OUT_DIR}/logfile_{CL_Stage}.txt"]
        traci.start(sumoCmd)
        ego_index = 5 + epo % 20   # é€‰å–éšæœºè½¦é“ç¬¬indexè¾†å‡ºå‘çš„è½¦ä¸ºæˆ‘ä»¬çš„è‡ªåŠ¨é©¾é©¶è½¦
        ego_index_str = str(np.random.randint(0,5)) + '_' + str(ego_index) # egoçš„idä¸º'1_$index$', å¦‚indexä¸º20,id='1_20'
        
        if CL_Stage == 1:
            target_dir_init = 1 # 3æ¡target lanesï¼Œ target_dirä¸ºç›´è¡Œ
        elif CL_Stage == 2:
            target_dir_init = 2 # 2æ¡target lanesï¼Œ target_dirä¸ºå·¦è½¬
        elif CL_Stage == 3:
            target_dir_init = 0 # 1æ¡target lanesï¼Œ target_dirä¸ºå³è½¬
        elif CL_Stage in [4, 5, 6]:
            target_dir_init = random.randint(0, 2) # éšæœºtarget_dir
            
        control_vehicle = '' # egoè½¦è¾†çš„id
        ego_show = False # egoè½¦è¾†æ˜¯å¦å‡ºç°è¿‡

        global df_record
        df_record = pd.DataFrame(columns = cols)
        
        print(f"+++++++{epo}  STAGE:{CL_Stage} +++++++++++++")
        print(f"++++++++++++++++++ {OUT_DIR} +++++++++++++++++++++++")
        
        # (4) ä¸€ä¸ªepisodeä¸­çš„äº¤äº’
        while traci.simulation.getMinExpectedNumber() > 0:
            # 1. å¾—åˆ°é“è·¯ä¸Šæ‰€æœ‰çš„è½¦è¾†ID
            vehicle_list = traci.vehicle.getIDList()
            
            # 2. æ‰¾åˆ°æˆ‘ä»¬æ§åˆ¶çš„è‡ªåŠ¨é©¾é©¶è½¦è¾†
            # 2.1 å¦‚æœæ­¤æ—¶è‡ªåŠ¨é©¾é©¶è½¦è¾†å·²å‡ºç°ï¼Œè®¾ç½®å…¶ä¸ºç»¿è‰², idä¸º'1_$ego_index$'
            if ego_index_str in vehicle_list:
                control_vehicle = ego_index_str
                ego_show = True
                traci.vehicle.setColor(control_vehicle,  (0,225,0,255))
            # 2.2 å¦‚æœæ­¤æ—¶è‡ªåŠ¨é©¾é©¶è½¦è¾†è¿˜æœªå‡ºç°
            if ego_show == False:
                traci.simulationStep() # 2ä¸ªstepå‡ºç°1è¾†è½¦
                continue
            # 2.3 å¦‚æœå·²ç»å‡ºç°äº†è€Œä¸”æ’äº†å°±é€€å‡º
            if ego_show and control_vehicle not in vehicle_list:
                print("=====================å·²ç»å‡ºç°äº†è€Œä¸”æ’äº†================")
                break
            
            # 3 åœ¨éRLæ§åˆ¶è·¯æ®µä¸­é‡‡å–å…¶ä»–è¡Œé©¶ç­–ç•¥ï¼Œæ§åˆ¶çš„è·¯æ®µä¸ºRL_CONTROL-3100è¿™2000mçš„è·ç¦»
            # 3.1 åœ¨0-RL_CONTROLmæ˜¯å»æ‰æ¨¡æ‹Ÿå™¨è‡ªå¸¦ç®—æ³•ä¸­çš„å˜é“ï¼Œä½†æš‚æ—¶ä¿ç•™é€Ÿåº¦æ§åˆ¶
            traci.vehicle.setLaneChangeMode(control_vehicle, 0b000000000000)
            # print("è‡ªåŠ¨é©¾é©¶è½¦çš„ä½ç½®====================", traci.vehicle.getPosition(control_vehicle)[0])     
            if traci.vehicle.getPosition(control_vehicle)[0] < RL_CONTROL:
                traci.simulationStep()
                continue
            # 3.2 åœ¨å¤§äº3100m
            if traci.vehicle.getPosition(control_vehicle)[0] > 3100:
                print("=======================è·ç¦»è¶…è¿‡3100====================")
                break
    
            # 4 åœ¨RLæ§åˆ¶è·¯æ®µä¸­æ”¶é›†è‡ªåŠ¨é©¾é©¶è½¦å‘¨å›´è½¦è¾†çš„ä¿¡æ¯ï¼Œå¹¶è®¾ç½®å‘¨å›´è½¦è¾†
            # 4.2 å°†æ‰€æœ‰åæ–¹è½¦è¾†è®¾ç½®ä¸ºä¸å˜é“
            # for vehicle in vehicle_list:
            #     if traci.vehicle.getPosition(vehicle)[0] < traci.vehicle.getPosition(control_vehicle)[0]:
            #         traci.vehicle.setLaneChangeMode(vehicle, 0b000000000000)
    
            # 4.3 å»é™¤è‡ªåŠ¨é©¾é©¶è½¦é»˜è®¤çš„è·Ÿè½¦å’Œæ¢é“æ¨¡å‹ï¼Œä¸ºæ¨¡å‹è®­ç»ƒåšå‡†å¤‡
            traci.vehicle.setSpeedMode(control_vehicle, 00000)
            traci.vehicle.setLaneChangeMode(control_vehicle, 0b000000000000)
            
            # 5 æ¨¡å‹è®­ç»ƒ
            collision, loss_actor, _ = train(worker, lock, traj_q, agent_q, control_vehicle, epo,  target_dir_init, CL_Stage) # æ¨¡æ‹Ÿä¸€ä¸ªæ—¶é—´æ­¥
            if collision:
                truncated = True
                break

            if loss_actor is not None:
                losses_actor.append(loss_actor)
                losses_episode.append(loss_actor)
            
        # (5) åˆ¤æ–­åœ¨æŸä¸ªstageçš„è®­ç»ƒæƒ…å†µï¼Œæ”¶æ•›äº†å°±è¿›å…¥ä¸‹ä¸€ä¸ªstage
        # globals()['PRE_LANE']=None
        traci.close(wait=True)
        switch_cnt += 1
        if TRAIN and not truncated and switch_cnt >= swicth_min and np.average(losses_episode)<=0.05:
            if CL_Stage == 1:
                CL_Stage = 2
                switch_cnt = 0 # è¿›å…¥ä¸‹ä¸€ä¸ªstageï¼Œswitch_cntæ¸…0
            elif CL_Stage == 2:
                CL_Stage = 3
                switch_cnt = 0
            elif CL_Stage == 3:
                CL_Stage = 4
                switch_cnt = 0
            elif CL_Stage == 4:
                CL_Stage = 5
                switch_cnt = 0
            elif CL_Stage == 5:
                CL_Stage = 6
                switch_cnt = 0
            # elif CL_Stage == 6:
            #     CL_Stage = 1
            #     swicth_cnt = 0

        losses_episode.clear()
        
        # ä¿å­˜
        df_record.to_csv(f"{OUT_DIR}/df_record_epo_{epo}.csv", index = False)
        if TRAIN:
            if worker._learn_step > 250000:
                torch.save(worker.state_dict(), f"./{OUT_DIR}/250000_net_params.pth")
            elif worker._learn_step > 200000:
                torch.save(worker.state_dict(), f"./{OUT_DIR}/200000_net_params.pth")
            elif worker._learn_step > 150000:
                torch.save(worker.state_dict(), f"./{OUT_DIR}/150000_net_params.pth")
            elif worker._learn_step > 100000:
                torch.save(worker.state_dict(), f"./{OUT_DIR}/100000_net_params.pth")
            elif worker._learn_step > 50000:
                torch.save(worker.state_dict(), f"./{OUT_DIR}/50000_net_params.pth")
            elif worker._learn_step > 20000:
                torch.save(worker.state_dict(), f"./{OUT_DIR}/20000_net_params.pth")
            torch.save(worker.state_dict(), f"./{OUT_DIR}/net_params.pth") 
            pd.DataFrame(data=losses_actor).to_csv(f"./{OUT_DIR}/losses.csv")

    [p.join() for p in process]

def learner_process(lock:Lock, traj_q: Queue, agent_q: Queue, agent_param:dict):
    learner = PDQNAgent(
        state_dim=agent_param["s_dim"],
        action_dim=agent_param["a_dim"],
        acc3=agent_param["acc3"],
        Kaiming_normal=agent_param["Kaiming_normal"],
        memory_size=agent_param["memory_size"],
        minimal_size=agent_param["minimal_size"],
        batch_size=agent_param["batch_size"],
        n_step=agent_param["n_step"],
        per_flag=agent_param["per_flag"],
        device=agent_param["device"])
    if TRAIN and os.path.exists(f"./model_params/{OUT_DIR}_net_params.pth"):
        learner.load_state_dict(torch.load(f"./model_params/{OUT_DIR}_net_params.pth", map_location=DEVICE))
    
    while(True):
        #k=max(len(learner.memory)//learner.minimal_size, 1)
        #learner.batch_size*=k
        for _ in range(UPDATE_FREQ):
            transition=traj_q.get(block=True, timeout=None)
            obs, tl_code, action, action_param, reward, next_obs, next_tl_code, done = transition[0], transition[1], transition[2], \
                transition[3], transition[4], transition[5], transition[6], transition[7]
            learner.store_transition(obs, tl_code, action, action_param, reward, next_obs, next_tl_code, done)

        if TRAIN and len(learner.memory)>=learner.minimal_size:
            print("LEARN BEGIN")
            for _ in range(UPDATE_FREQ):
                loss_actor, Q_loss=learner.learn()
            #loss_actor, Q_loss=[learner.learn() for _ in range(k)]
            if not agent_q.full() and learner._learn_step % UPDATE_FREQ == 0:
                # actor=deepcopy(learner.actor.state_dict())
                # actor_target=deepcopy(learner.actor_target.state_dict())
                # param=deepcopy(learner.param.state_dict())
                # param_target=deepcopy(learner.param_target.state_dict())
                
                lock.acquire()
                agent_q.put((deepcopy(learner._learn_step), deepcopy(loss_actor), deepcopy(Q_loss)), block=True, timeout=None)
                torch.save({
                    "actor":learner.actor.state_dict(),
                    "actor_target":learner.actor_target.state_dict(),
                    "param":learner.param.state_dict(),
                    "param_target":learner.param_target.state_dict()
                }, f"./{OUT_DIR}/learner.pth")
                lock.release()
                #agent_q.put((actor, actor_target, param, param_target, learner._learn_step, loss_actor, Q_loss), block=True, timeout=None)

if __name__ == '__main__':
    mp.set_start_method(method="spawn", force=True)
    main_train() 
#    pass




